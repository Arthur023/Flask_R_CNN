from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import random
import pprint
import shutil
import sys
import time
import numpy as np
# from optparse import OptionParser
import pickle
import math
import cv2
import copy

# from app import Config as C

# from PIL import Image

from flask import send_file
from matplotlib import pyplot as plt
import tensorflow as tf
# tf.disable_v2_behavior()

# import pandas as pd

# from sklearn.metrics import average_precision_score

# from sklearn.metrics import average_precision_score, precision_recall_curve


from tensorflow import keras

from tensorflow.keras import backend as K

# from keras.optimizers import Adam, SGD, RMSprop

# from tensorflow.keras.models import load_model

from tensorflow.keras.layers import Flatten, Dense, Input, MaxPooling2D, Dropout, Activation, ZeroPadding2D, \
    BatchNormalization, \
    Conv2D, Add
from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed, AveragePooling2D
# from keras.engine.topology import get_source_inputs
# from keras.utils import layer_utils
# from keras.utils.data_utils import get_file
# from keras.objectives import categorical_crossentropy
from tensorflow.keras.initializers import glorot_uniform

from tensorflow.keras.models import Model
# from keras.utils import generic_utils
from tensorflow.keras.layers import Layer

# from keras import initializers, regularizers

from zipfile import ZipFile


class Config:

    def __init__(self):
        # Print the process or not
        self.verbose = True

        # Name of base network
        self.network = 'resnet50'

        # Setting for data augmentation
        self.use_horizontal_flips = False
        self.use_vertical_flips = False
        self.rot_90 = False

        # Anchor box scales
        # Note that if im_size is smaller, anchor_box_scales should be scaled
        # Original anchor_box_scales in the paper is [128, 256, 512]
        self.anchor_box_scales = [64, 128, 256]  # [16, 32, 64]

        # Anchor box ratios
        self.anchor_box_ratios = [[1, 1], [0.75, 0.75], [1.25, 1.25]]

        # Size to resize the smallest side of the image
        # Original setting in paper is 600. Set to 300 in here to save training time
        self.im_size = 600

        # image channel-wise mean to subtract
        self.img_channel_mean = [103.939, 116.779, 123.68]
        self.img_scaling_factor = 1.0

        # number of ROIs at once
        self.num_rois = 16

        # stride at the RPN (this depends on the network configuration)
        self.rpn_stride = 16

        self.balanced_classes = False

        # scaling the stdev
        self.std_scaling = 4.0
        self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]

        # overlaps for RPN
        self.rpn_min_overlap = 0.3
        self.rpn_max_overlap = 0.7

        # overlaps for classifier ROIs
        self.classifier_min_overlap = 0.1
        self.classifier_max_overlap = 0.5

        # placeholder for the class mapping, automatically generated by the parser
        self.class_mapping = {'huidmond': 0, 'bg': 1}

        self.model_path = 'model_test/model_frcnn_resnet.hdf5'


class RoiPoolingConv(Layer):
    '''ROI pooling layer for 2D inputs.
    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,
    K. He, X. Zhang, S. Ren, J. Sun
    # Arguments
        pool_size: int
            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.
        num_rois: number of regions of interest to be used
    # Input shape
        list of two 4D tensors [X_img,X_roi] with shape:
        X_img:
        `(1, rows, cols, channels)`
        X_roi:
        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)
    # Output shape
        3D tensor with shape:
        `(1, num_rois, channels, pool_size, pool_size)`
    '''

    def __init__(self, pool_size, num_rois, **kwargs):
        self.dim_ordering = 'tf'
        self.pool_size = pool_size
        self.num_rois = num_rois

        super(RoiPoolingConv, self).__init__(**kwargs)

    def build(self, input_shape):
        self.nb_channels = input_shape[0][3]

    def compute_output_shape(self, input_shape):
        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels

    def call(self, x, mask=None):
        assert (len(x) == 2)

        # x[0] is image with shape (rows, cols, channels)
        img = x[0]

        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)
        rois = x[1]

        input_shape = K.shape(img)

        outputs = []

        for roi_idx in range(self.num_rois):
            x = rois[0, roi_idx, 0]
            y = rois[0, roi_idx, 1]
            w = rois[0, roi_idx, 2]
            h = rois[0, roi_idx, 3]

            x = K.cast(x, 'int32')
            y = K.cast(y, 'int32')
            w = K.cast(w, 'int32')
            h = K.cast(h, 'int32')

            # Resized roi of the image to pooling size (7x7)
            rs = tf.image.resize(img[:, y:y + h, x:x + w, :], (self.pool_size, self.pool_size))
            outputs.append(rs)

        final_output = K.concatenate(outputs, axis=0)

        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)
        # Might be (1, 4, 7, 7, 3)
        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))

        # permute_dimensions is similar to transpose
        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))

        return final_output

    def get_config(self):
        config = {'pool_size': self.pool_size,
                  'num_rois': self.num_rois}
        base_config = super(RoiPoolingConv, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


# identity_block
#
# def identity_block(input_tensor, kernel_size, filters, stage, block, trainable=True):
#     """
#     Implementation of the identity block as defined in Figure 3
#
#     Arguments:
#     input_tensor -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)
#     kernel_size -- integer, specifying the shape of the middle CONV's window for the main path
#     filters -- python list of integers, defining the number of filters in the CONV layers of the main path
#     stage -- integer, used to name the layers, depending on their position in the network
#     block -- string/character, used to name the layers, depending on their position in the network
#
#     Returns:
#     X -- output of the identity block, tensor of shape (n_H, n_W, n_C)
#     """
#
#     # defining name basis
#     conv_name_base = 'res' + str(stage) + block + '_branch'
#     bn_name_base = 'bn' + str(stage) + block + '_branch'
#
#     # Retrieve Filters
#     F1, F2, F3 = filters
#
#     # First component of main path
#     X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a',
#                kernel_initializer=glorot_uniform(seed=0))(input_tensor)
#     X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)
#     X = Activation('relu')(X)
#
#     # Second component of main path
#     X = Conv2D(filters=F2, kernel_size=(kernel_size, kernel_size), strides=(1, 1), padding='same',
#                name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)
#     X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)
#     X = Activation('relu')(X)
#
#     # Third component of main path
#     X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c',
#                kernel_initializer=glorot_uniform(seed=0))(X)
#     X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)
#
#     # Final step: Add shortcut value to main path, and pass it through a RELU activation
#     X = Add()([X, input_tensor])
#     X = Activation('relu')(X)
#
#     return X
#
#
# def identity_block_td(input_tensor, kernel_size, filters, stage, block, trainable=True):
#     """
#     Implementation of the identity block as defined in Figure 3
#
#     Arguments:
#     input_tensor -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)
#     kernel_size -- integer, specifying the shape of the middle CONV's window for the main path
#     filters -- python list of integers, defining the number of filters in the CONV layers of the main path
#     stage -- integer, used to name the layers, depending on their position in the network
#     block -- string/character, used to name the layers, depending on their position in the network
#
#     Returns:
#     X -- output of the identity block, tensor of shape (n_H, n_W, n_C)
#     """
#
#     # defining name basis
#     conv_name_base = 'res' + str(stage) + block + '_branch'
#     bn_name_base = 'bn' + str(stage) + block + '_branch'
#
#     # Retrieve Filters
#     F1, F2, F3 = filters
#
#     # First component of main path
#     X = TimeDistributed(Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid',
#                                kernel_initializer=glorot_uniform(seed=0)), name=conv_name_base + '2a')(input_tensor)
#     X = TimeDistributed(BatchNormalization(axis=3), name=bn_name_base + '2a')(X)
#     X = Activation('relu')(X)
#
#     # Second component of main path
#     X = TimeDistributed(Conv2D(filters=F2, kernel_size=(kernel_size, kernel_size), strides=(1, 1), padding='same',
#                                kernel_initializer=glorot_uniform(seed=0)), name=conv_name_base + '2b')(X)
#     X = TimeDistributed(BatchNormalization(axis=3), name=bn_name_base + '2b')(X)
#     X = Activation('relu')(X)
#
#     # Third component of main path
#     X = TimeDistributed(Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid',
#                                kernel_initializer=glorot_uniform(seed=0)), name=conv_name_base + '2c')(X)
#     X = TimeDistributed(BatchNormalization(axis=3), name=bn_name_base + '2c')(X)
#
#     # Final step: Add shortcut value to main path, and pass it through a RELU activation
#     X = Add()([X, input_tensor])
#     X = Activation('relu')(X)
#
#     return X
#
#
# def convolutional_block(input_tensor, f, filters, stage, block, s=2):
#     """
#     Implementation of the convolutional block as defined in Figure 4
#
#     Arguments:
#     input_tensor -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)
#     f -- integer, specifying the shape of the middle CONV's window for the main path
#     filters -- python list of integers, defining the number of filters in the CONV layers of the main path
#     stage -- integer, used to name the layers, depending on their position in the network
#     block -- string/character, used to name the layers, depending on their position in the network
#     s -- Integer, specifying the stride to be used
#
#     Returns:
#     X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)
#     """
#
#     # defining name basis
#     conv_name_base = 'res' + str(stage) + block + '_branch'
#     bn_name_base = 'bn' + str(stage) + block + '_branch'
#
#     # Retrieve Filters
#     F1, F2, F3 = filters
#
#     # HIER Conv2D(F1,(3,3)) veranderd naar (1,1)
#     # First component of main path
#     X = Conv2D(F1, (1, 1), strides=(s, s), padding='same', name=conv_name_base + '2a',
#                kernel_initializer=glorot_uniform(seed=0))(input_tensor)
#     X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)
#     X = Activation('relu')(X)
#
#     # Second component of main path
#     X = Conv2D(F2, (f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b',
#                kernel_initializer=glorot_uniform(seed=0))(X)
#     X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)
#     X = Activation('relu')(X)
#
#     # Third component of main path
#     X = Conv2D(F3, (1, 1), strides=(1, 1), padding='same', name=conv_name_base + '2c',
#                kernel_initializer=glorot_uniform(seed=0))(X)
#     X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)
#
#     X_shortcut = Conv2D(F3, (1, 1), strides=(s, s), padding='same', name=conv_name_base + '1',
#                         kernel_initializer=glorot_uniform(seed=0))(input_tensor)
#     X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)
#
#     # Final step: Add shortcut value to main path, and pass it through a RELU activation
#     X = Add()([X, X_shortcut])
#     X = Activation('relu')(X)
#
#     return X
#
#
# def convolutional_block_td(input_tensor, f, filters, stage, block, s=2):
#     """
#     Implementation of the convolutional block as defined in Figure 4
#
#     Arguments:
#     input_tensor -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)
#     f -- integer, specifying the shape of the middle CONV's window for the main path
#     filters -- python list of integers, defining the number of filters in the CONV layers of the main path
#     stage -- integer, used to name the layers, depending on their position in the network
#     block -- string/character, used to name the layers, depending on their position in the network
#     s -- Integer, specifying the stride to be used
#
#     Returns:
#     X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)
#     """
#
#     # defining name basis
#     conv_name_base = 'res' + str(stage) + block + '_branch'
#     bn_name_base = 'bn' + str(stage) + block + '_branch'
#
#     bn_axis = 3
#
#     # Retrieve Filters
#     F1, F2, F3 = filters
#
#     # First component of main path
#     X = TimeDistributed(Conv2D(F1, (1, 1), strides=(s, s), padding='same', kernel_initializer=glorot_uniform(seed=0)),
#                         name=conv_name_base + '2a')(input_tensor)
#     X = TimeDistributed(BatchNormalization(axis=3), name=bn_name_base + '2a')(X)
#     X = Activation('relu')(X)
#
#     # Second component of main path
#     X = TimeDistributed(Conv2D(F2, (f, f), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0)),
#                         name=conv_name_base + '2b')(X)
#     X = TimeDistributed(BatchNormalization(axis=3), name=bn_name_base + '2b')(X)
#     X = Activation('relu')(X)
#
#     # Third component of main path
#     X = TimeDistributed(Conv2D(F3, (1, 1), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0)),
#                         name=conv_name_base + '2c')(X)
#     X = TimeDistributed(BatchNormalization(axis=3), name=bn_name_base + '2c')(X)
#
#     X_shortcut = TimeDistributed(
#         Conv2D(F3, (1, 1), strides=(s, s), padding='same', kernel_initializer=glorot_uniform(seed=0)),
#         name=conv_name_base + '1')(input_tensor)
#     X_shortcut = TimeDistributed(BatchNormalization(axis=3), name=bn_name_base + '1')(X_shortcut)
#
#     # Final step: Add shortcut value to main path, and pass it through a RELU activation
#     X = Add()([X, X_shortcut])
#     X = Activation('relu')(X)
#
#     return X
#
#
# def get_img_output_length(width, height):
#     # function to calculate final layer's feature map (of base model) size according to input image size
#     def get_output_length(input_length):
#         input_float = input_length + 0.0
#         test = math.ceil(input_float / 16)
#         # //16 omdat we 4 lagen hebben die telkens halveren (/2)
#         # test = input_length//16
#         # print('input_float',input_float)
#         # print('test',test)
#         return test
#
#     return get_output_length(width), get_output_length(height)
#
#
# def nn_base(input_tensor=None, trainable=False):
#     input_shape = (None, None, 3)
#
#     classes = 1
#
#     if input_tensor is None:
#         img_input = Input(shape=input_shape)
#     else:
#         if not K.is_keras_tensor(input_tensor):
#             img_input = Input(tensor=input_tensor, shape=input_shape)
#         else:
#             img_input = input_tensor
#     bn_axis = 3
#
#     # Zero-Padding
#     X = ZeroPadding2D((3, 3))(img_input)
#
#     # Stage 1
#     X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)
#     X = BatchNormalization(axis=3, name='bn_conv1')(X)
#     X = Activation('relu')(X)
#     X = MaxPooling2D((3, 3), strides=(2, 2))(X)
#
#     # Stage 2
#     X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)
#     X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')
#     X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')
#
#     ### START CODE HERE ###
#
#     # Stage 3 (4 lines)
#     X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)
#     X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')
#     X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')
#     X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')
#
#     # Stage 4 (6 lines)
#     # X = MaxPooling2D((2, 2), strides=(2, 2), name='BLOCK_4')(X)
#     X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)
#     X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')
#     X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')
#     X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')
#     X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')
#     X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')
#
#     # Stage 5 (3 lines)
#     # X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)
#     # X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')
#     # X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')
#
#     # AVGPOOL (1 line). Use "X = AveragePooling2D(...)(X)"
#     # X = AveragePooling2D((2,2), name="avg_pool")(X)
#
#     ### END CODE HERE ###
#
#     # output layer
#     ####################################
#     # Dit moet nog uit commentaar
#     # Alleen de Flatten
#     # X = Flatten()(X)
#
#     # X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)
#     # Create model
#     # model = Model(inputs = X_input, outputs = X, name='ResNet50')
#
#     return X
#
#
# def rpn_layer(base_layers, num_anchors):
#     """Create a rpn layer
#         Step1: Pass through the feature map from base layer to a 3x3 512 channels convolutional layer
#                 Keep the padding 'same' to preserve the feature map's size
#         Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer
#                 classification layer: num_anchors (9 in here) channels for 0, 1 sigmoid activation output
#                 regression layer: num_anchors*4 (36 in here) channels for computing the regression of bboxes with linear activation
#     Args:
#         base_layers: vgg in here
#         num_anchors: 9 in here
#
#     Returns:
#         [x_class, x_regr, base_layers]
#         x_class: classification for whether it's an object
#         x_regr: bboxes regression
#         base_layers: vgg in here
#     """
#
#     x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(
#         base_layers)
#
#     x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)
#     x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)
#
#     return [x_class, x_regr, base_layers]
#
#
# def classifier_layers(X, input_shape):
#     # compile times on theano tend to be very high, so we use smaller ROI pooling regions to workaround
#     # (hence a smaller stride in the region that follows the ROI pool)
#     # X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)
#
#     X = convolutional_block_td(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)
#
#     X = identity_block_td(X, 3, [512, 512, 2048], stage=5, block='b')
#     X = identity_block_td(X, 3, [512, 512, 2048], stage=5, block='c')
#     X = TimeDistributed(AveragePooling2D((2, 2)), name='avg_pool')(X)
#
#     return X
#
#
# def classifier_model(base_layers, input_rois, num_rois, nb_classes=4):
#     """Create a classifier layer
#
#     Args:
#         base_layers: vgg
#         input_rois: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)
#         num_rois: number of rois to be processed in one time (4 in here)
#
#     Returns:
#         list(out_class, out_regr)
#         out_class: classifier layer output
#         out_regr: regression layer output
#     """
#
#     input_shape = (num_rois, 7, 7, 512)
#
#     pooling_regions = 7
#
#     # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)
#     # num_rois (4) 7x7 roi pooling
#     out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])
#
#     out = classifier_layers(out_roi_pool, input_shape=input_shape)
#
#     # Flatten the convlutional layer and connected to 2 FC and 2 dropout
#     out = TimeDistributed(Flatten(name='flatten'))(out)
#     out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)
#     out = TimeDistributed(Dropout(0.5))(out)
#     out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)
#     out = TimeDistributed(Dropout(0.5))(out)
#
#     # There are two output layer
#     # out_class: softmax acivation function for classify the class name of the object
#     # out_regr: linear activation function for bboxes coordinates regression
#     out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'),
#                                 name='dense_class_{}'.format(nb_classes))(out)
#     # note: no regression target for bg class
#     out_regr = TimeDistributed(Dense(4 * (nb_classes - 1), activation='linear', kernel_initializer='zero'),
#                                name='dense_regress_{}'.format(nb_classes))(out)
#
#     return [out_class, out_regr]


def union(au, bu, area_intersection):
    area_a = (au[2] - au[0]) * (au[3] - au[1])
    area_b = (bu[2] - bu[0]) * (bu[3] - bu[1])
    area_union = area_a + area_b - area_intersection
    return area_union


def intersection(ai, bi):
    x = max(ai[0], bi[0])
    y = max(ai[1], bi[1])
    w = min(ai[2], bi[2]) - x
    h = min(ai[3], bi[3]) - y
    if w < 0 or h < 0:
        return 0
    return w * h


def iou(a, b):
    # a and b should be (x1,y1,x2,y2)

    if a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:
        return 0.0

    area_i = intersection(a, b)
    area_u = union(a, b, area_i)

    return float(area_i) / float(area_u + 1e-6)


def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):
    """(Important part!) Calculate the rpn for all anchors
        If feature map has shape 38x50=1900, there are 1900x9=17100 potential anchors

    Args:
        C: config
        img_data: augmented image data
        width: original image width (e.g. 600)
        height: original image height (e.g. 800)
        resized_width: resized image width according to C.im_size (e.g. 300)
        resized_height: resized image height according to C.im_size (e.g. 400)
        img_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size

    Returns:
        y_rpn_cls: list(num_bboxes, y_is_box_valid + y_rpn_overlap)
            y_is_box_valid: 0 or 1 (0 means the box is invalid, 1 means the box is valid)
            y_rpn_overlap: 0 or 1 (0 means the box is not an object, 1 means the box is an object)
        y_rpn_regr: list(num_bboxes, 4*y_rpn_overlap + y_rpn_regr)
            y_rpn_regr: x1,y1,x2,y2 bunding boxes coordinates
    """
    downscale = float(C.rpn_stride)
    anchor_sizes = C.anchor_box_scales  # 128, 256, 512
    anchor_ratios = C.anchor_box_ratios  # 1:1, 1:2*sqrt(2), 2*sqrt(2):1
    num_anchors = len(anchor_sizes) * len(anchor_ratios)  # 3x3=9

    # calculate the output map size based on the network architecture
    (output_width, output_height) = img_length_calc_function(resized_width, resized_height)

    n_anchratios = len(anchor_ratios)  # 3

    # initialise empty output objectives
    y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))
    y_is_box_valid = np.zeros((output_height, output_width, num_anchors))
    y_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))

    num_bboxes = len(img_data['bboxes'])

    num_anchors_for_bbox = np.zeros(num_bboxes).astype(int)
    best_anchor_for_bbox = -1 * np.ones((num_bboxes, 4)).astype(int)
    best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)
    best_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)
    best_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)

    # get the GT box coordinates, and resize to account for image resizing
    gta = np.zeros((num_bboxes, 4))
    for bbox_num, bbox in enumerate(img_data['bboxes']):
        # get the GT box coordinates, and resize to account for image resizing
        gta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))
        gta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))
        gta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))
        gta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))

    # rpn ground truth

    for anchor_size_idx in range(len(anchor_sizes)):
        for anchor_ratio_idx in range(n_anchratios):
            anchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]
            anchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]

            for ix in range(output_width):
                # x-coordinates of the current anchor box
                x1_anc = downscale * (ix + 0.5) - anchor_x / 2
                x2_anc = downscale * (ix + 0.5) + anchor_x / 2

                # ignore boxes that go across image boundaries
                if x1_anc < 0 or x2_anc > resized_width:
                    continue

                for jy in range(output_height):

                    # y-coordinates of the current anchor box
                    y1_anc = downscale * (jy + 0.5) - anchor_y / 2
                    y2_anc = downscale * (jy + 0.5) + anchor_y / 2

                    # ignore boxes that go across image boundaries
                    if y1_anc < 0 or y2_anc > resized_height:
                        continue

                    # bbox_type indicates whether an anchor should be a target
                    # Initialize with 'negative'
                    bbox_type = 'neg'

                    # this is the best IOU for the (x,y) coord and the current anchor
                    # note that this is different from the best IOU for a GT bbox
                    best_iou_for_loc = 0.0

                    for bbox_num in range(num_bboxes):

                        # get IOU of the current GT box and the current anchor box
                        curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]],
                                       [x1_anc, y1_anc, x2_anc, y2_anc])
                        # calculate the regression targets if they will be needed
                        if curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:
                            cx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0
                            cy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0
                            cxa = (x1_anc + x2_anc) / 2.0
                            cya = (y1_anc + y2_anc) / 2.0

                            # x,y are the center point of ground-truth bbox
                            # xa,ya are the center point of anchor bbox (xa=downscale * (ix + 0.5); ya=downscale * (iy+0.5))
                            # w,h are the width and height of ground-truth bbox
                            # wa,ha are the width and height of anchor bboxe
                            # tx = (x - xa) / wa
                            # ty = (y - ya) / ha
                            # tw = log(w / wa)
                            # th = log(h / ha)
                            tx = (cx - cxa) / (x2_anc - x1_anc)
                            ty = (cy - cya) / (y2_anc - y1_anc)
                            tw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))
                            th = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))

                        if img_data['bboxes'][bbox_num]['class'] != 'bg':

                            # all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best
                            if curr_iou > best_iou_for_bbox[bbox_num]:
                                best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]
                                best_iou_for_bbox[bbox_num] = curr_iou
                                best_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]
                                best_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]

                            # we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)
                            if curr_iou > C.rpn_max_overlap:
                                bbox_type = 'pos'
                                num_anchors_for_bbox[bbox_num] += 1
                                # we update the regression layer target if this IOU is the best for the current (x,y) and anchor position
                                if curr_iou > best_iou_for_loc:
                                    best_iou_for_loc = curr_iou
                                    best_regr = (tx, ty, tw, th)

                            # if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective
                            if C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:
                                # gray zone between neg and pos
                                if bbox_type != 'pos':
                                    bbox_type = 'neutral'

                    # turn on or off outputs depending on IOUs
                    if bbox_type == 'neg':
                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1
                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0
                    elif bbox_type == 'neutral':
                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0
                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0
                    elif bbox_type == 'pos':
                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1
                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1
                        start = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)
                        y_rpn_regr[jy, ix, start:start + 4] = best_regr

    # we ensure that every bbox has at least one positive RPN region

    for idx in range(num_anchors_for_bbox.shape[0]):
        if num_anchors_for_bbox[idx] == 0:
            # no box with an IOU greater than zero ...
            if best_anchor_for_bbox[idx, 0] == -1:
                continue
            y_is_box_valid[
                best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1], best_anchor_for_bbox[
                    idx, 2] + n_anchratios *
                best_anchor_for_bbox[idx, 3]] = 1
            y_rpn_overlap[
                best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1], best_anchor_for_bbox[
                    idx, 2] + n_anchratios *
                best_anchor_for_bbox[idx, 3]] = 1
            start = 4 * (best_anchor_for_bbox[idx, 2] + n_anchratios * best_anchor_for_bbox[idx, 3])
            y_rpn_regr[
            best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1], start:start + 4] = best_dx_for_bbox[idx, :]

    y_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))
    y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)

    y_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))
    y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)

    y_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))
    y_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)

    pos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))
    neg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))

    num_pos = len(pos_locs[0])

    # one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative
    # regions. We also limit it to 256 regions.
    num_regions = 256

    if len(pos_locs[0]) > num_regions / 2:
        val_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions / 2)
        y_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0
        num_pos = num_regions / 2

    if len(neg_locs[0]) + num_pos > num_regions:
        val_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)
        y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0

    y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)
    y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)

    return np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos


def get_new_img_size(width, height, img_min_side=600):
    if width <= height:
        f = float(img_min_side) / width
        resized_height = int(f * height)
        resized_width = img_min_side
    else:
        f = float(img_min_side) / height
        resized_width = int(f * width)
        resized_height = img_min_side

    return resized_width, resized_height


def augment(img_data, config, augment=True):
    assert 'filepath' in img_data
    assert 'bboxes' in img_data
    assert 'width' in img_data
    assert 'height' in img_data

    img_data_aug = copy.deepcopy(img_data)

    img = cv2.imread(img_data_aug['filepath'])

    if augment:
        rows, cols = img.shape[:2]

        if config.use_horizontal_flips and np.random.randint(0, 2) == 0:
            img = cv2.flip(img, 1)
            for bbox in img_data_aug['bboxes']:
                x1 = bbox['x1']
                x2 = bbox['x2']
                bbox['x2'] = cols - x1
                bbox['x1'] = cols - x2

        if config.use_vertical_flips and np.random.randint(0, 2) == 0:
            img = cv2.flip(img, 0)
            for bbox in img_data_aug['bboxes']:
                y1 = bbox['y1']
                y2 = bbox['y2']
                bbox['y2'] = rows - y1
                bbox['y1'] = rows - y2

        if config.rot_90:
            angle = np.random.choice([0, 90, 180, 270], 1)[0]
            if angle == 270:
                img = np.transpose(img, (1, 0, 2))
                img = cv2.flip(img, 0)
            elif angle == 180:
                img = cv2.flip(img, -1)
            elif angle == 90:
                img = np.transpose(img, (1, 0, 2))
                img = cv2.flip(img, 1)
            elif angle == 0:
                pass

            for bbox in img_data_aug['bboxes']:
                x1 = bbox['x1']
                x2 = bbox['x2']
                y1 = bbox['y1']
                y2 = bbox['y2']
                if angle == 270:
                    bbox['x1'] = y1
                    bbox['x2'] = y2
                    bbox['y1'] = cols - x2
                    bbox['y2'] = cols - x1
                elif angle == 180:
                    bbox['x2'] = cols - x1
                    bbox['x1'] = cols - x2
                    bbox['y2'] = rows - y1
                    bbox['y1'] = rows - y2
                elif angle == 90:
                    bbox['x1'] = rows - y2
                    bbox['x2'] = rows - y1
                    bbox['y1'] = x1
                    bbox['y2'] = x2
                elif angle == 0:
                    pass

    img_data_aug['width'] = img.shape[1]
    img_data_aug['height'] = img.shape[0]
    return img_data_aug, img


def get_anchor_gt(all_img_data, C, img_length_calc_function, mode='train'):
    """ Yield the ground-truth anchors as Y (labels)

    Args:
        all_img_data: list(filepath, width, height, list(bboxes))
        C: config
        img_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size
        mode: 'train' or 'test'; 'train' mode need augmentation

    Returns:
        x_img: image data after resized and scaling (smallest size = 300px)
        Y: [y_rpn_cls, y_rpn_regr]
        img_data_aug: augmented image data (original image with augmentation)
        debug_img: show image for debug
        num_pos: show number of positive anchors for debug
    """
    while True:

        for img_data in all_img_data:
            try:

                # read in image, and optionally add augmentation

                if mode == 'train':
                    img_data_aug, x_img = augment(img_data, C, augment=True)
                else:
                    img_data_aug, x_img = augment(img_data, C, augment=False)

                (width, height) = (img_data_aug['width'], img_data_aug['height'])
                (rows, cols, _) = x_img.shape

                assert cols == width
                assert rows == height

                # get image dimensions for resizing
                (resized_width, resized_height) = get_new_img_size(width, height, C.im_size)

                # resize the image so that smalles side is length = 300px
                x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)
                debug_img = x_img.copy()

                try:
                    y_rpn_cls, y_rpn_regr, num_pos = calc_rpn(C, img_data_aug, width, height, resized_width,
                                                              resized_height, img_length_calc_function)
                except:
                    continue

                # Zero-center by mean pixel, and preprocess image

                x_img = x_img[:, :, (2, 1, 0)]  # BGR -> RGB
                x_img = x_img.astype(np.float32)
                x_img[:, :, 0] -= C.img_channel_mean[0]
                x_img[:, :, 1] -= C.img_channel_mean[1]
                x_img[:, :, 2] -= C.img_channel_mean[2]
                x_img /= C.img_scaling_factor

                x_img = np.transpose(x_img, (2, 0, 1))
                x_img = np.expand_dims(x_img, axis=0)

                y_rpn_regr[:, y_rpn_regr.shape[1] // 2:, :, :] *= C.std_scaling

                x_img = np.transpose(x_img, (0, 2, 3, 1))
                y_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))
                y_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))

                yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug, debug_img, num_pos

            except Exception as e:
                print(e)
                continue


def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=600):
    # code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/
    # if there are no boxes, return an empty list

    # Process explanation:
    #   Step 1: Sort the probs list
    #   Step 2: Find the larget prob 'Last' in the list and save it to the pick list
    #   Step 3: Calculate the IoU with 'Last' box and other boxes in the list. If the IoU is larger than overlap_threshold, delete the box from list
    #   Step 4: Repeat step 2 and step 3 until there is no item in the probs list
    if len(boxes) == 0:
        return []

    # grab the coordinates of the bounding boxes
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]

    np.testing.assert_array_less(x1, x2)
    np.testing.assert_array_less(y1, y2)

    # if the bounding boxes integers, convert them to floats --
    # this is important since we'll be doing a bunch of divisions
    if boxes.dtype.kind == "i":
        boxes = boxes.astype("float")

    # initialize the list of picked indexes
    pick = []

    # calculate the areas
    area = (x2 - x1) * (y2 - y1)

    # sort the bounding boxes
    idxs = np.argsort(probs)

    # keep looping while some indexes still remain in the indexes
    # list
    while len(idxs) > 0:
        # grab the last index in the indexes list and add the
        # index value to the list of picked indexes
        last = len(idxs) - 1
        i = idxs[last]
        pick.append(i)

        # find the intersection

        xx1_int = np.maximum(x1[i], x1[idxs[:last]])
        yy1_int = np.maximum(y1[i], y1[idxs[:last]])
        xx2_int = np.minimum(x2[i], x2[idxs[:last]])
        yy2_int = np.minimum(y2[i], y2[idxs[:last]])

        ww_int = np.maximum(0, xx2_int - xx1_int)
        hh_int = np.maximum(0, yy2_int - yy1_int)

        area_int = ww_int * hh_int

        # find the union
        area_union = area[i] + area[idxs[:last]] - area_int

        # compute the ratio of overlap
        overlap = area_int / (area_union + 1e-6)

        # delete all indexes from the index list that have
        idxs = np.delete(idxs, np.concatenate(([last],
                                               np.where(overlap > overlap_thresh)[0])))

        if len(pick) >= max_boxes:
            break

    # return only the bounding boxes that were picked using the integer data type
    boxes = boxes[pick].astype("int")
    probs = probs[pick]
    return boxes, probs


def apply_regr_np(X, T):
    """Apply regression layer to all anchors in one feature map

    Args:
        X: shape=(4, 18, 25) the current anchor type for all points in the feature map
        T: regression layer shape=(4, 18, 25)

    Returns:
        X: regressed position and size for current anchor
    """
    try:
        x = X[0, :, :]
        y = X[1, :, :]
        w = X[2, :, :]
        h = X[3, :, :]

        tx = T[0, :, :]
        ty = T[1, :, :]
        tw = T[2, :, :]
        th = T[3, :, :]

        cx = x + w / 2.
        cy = y + h / 2.
        cx1 = tx * w + cx
        cy1 = ty * h + cy

        w1 = np.exp(tw.astype(np.float64)) * w
        h1 = np.exp(th.astype(np.float64)) * h
        x1 = cx1 - w1 / 2.
        y1 = cy1 - h1 / 2.

        x1 = np.round(x1)
        y1 = np.round(y1)
        w1 = np.round(w1)
        h1 = np.round(h1)
        return np.stack([x1, y1, w1, h1])
    except Exception as e:
        print(e)
        return X


def apply_regr(x, y, w, h, tx, ty, tw, th):
    # Apply regression to x, y, w and h
    try:
        cx = x + w / 2.
        cy = y + h / 2.
        cx1 = tx * w + cx
        cy1 = ty * h + cy
        w1 = math.exp(tw) * w
        h1 = math.exp(th) * h
        x1 = cx1 - w1 / 2.
        y1 = cy1 - h1 / 2.
        x1 = int(round(x1))
        y1 = int(round(y1))
        w1 = int(round(w1))
        h1 = int(round(h1))

        return x1, y1, w1, h1

    except ValueError:
        return x, y, w, h
    except OverflowError:
        return x, y, w, h
    except Exception as e:
        print(e)
        return x, y, w, h


def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=600, overlap_thresh=0.9):
    """Convert rpn layer to roi bboxes

    Args: (num_anchors = 9)
        rpn_layer: output layer for rpn classification
            shape (1, feature_map.height, feature_map.width, num_anchors)
            Might be (1, 18, 25, 9) if resized image is 400 width and 300
        regr_layer: output layer for rpn regression
            shape (1, feature_map.height, feature_map.width, num_anchors)
            Might be (1, 18, 25, 36) if resized image is 400 width and 300
        C: config
        use_regr: Wether to use bboxes regression in rpn
        max_boxes: max bboxes number for non-max-suppression (NMS)
        overlap_thresh: If iou in NMS is larger than this threshold, drop the box

    Returns:
        result: boxes from non-max-suppression (shape=(300, 4))
            boxes: coordinates for bboxes (on the feature map)
    """
    regr_layer = regr_layer / C.std_scaling

    anchor_sizes = C.anchor_box_scales  # (3 in here)
    anchor_ratios = C.anchor_box_ratios  # (3 in here)

    assert rpn_layer.shape[0] == 1

    (rows, cols) = rpn_layer.shape[1:3]

    curr_layer = 0

    # A.shape = (4, feature_map.height, feature_map.width, num_anchors)
    # Might be (4, 18, 25, 9) if resized image is 400 width and 300
    # A is the coordinates for 9 anchors for every point in the feature map
    # => all 18x25x9=4050 anchors cooridnates
    A = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))

    for anchor_size in anchor_sizes:
        for anchor_ratio in anchor_ratios:
            # anchor_x = (128 * 1) / 16 = 8  => width of current anchor
            # anchor_y = (128 * 2) / 16 = 16 => height of current anchor
            anchor_x = (anchor_size * anchor_ratio[0]) / C.rpn_stride
            anchor_y = (anchor_size * anchor_ratio[1]) / C.rpn_stride

            # curr_layer: 0~8 (9 anchors)
            # the Kth anchor of all position in the feature map (9th in total)
            regr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]  # shape => (18, 25, 4)
            regr = np.transpose(regr, (2, 0, 1))  # shape => (4, 18, 25)

            # Create 18x25 mesh grid
            # For every point in x, there are all the y points and vice versa
            # X.shape = (18, 25)
            # Y.shape = (18, 25)
            X, Y = np.meshgrid(np.arange(cols), np.arange(rows))

            # Calculate anchor position and size for each feature map point
            A[0, :, :, curr_layer] = X - anchor_x / 2  # Top left x coordinate
            A[1, :, :, curr_layer] = Y - anchor_y / 2  # Top left y coordinate
            A[2, :, :, curr_layer] = anchor_x  # width of current anchor
            A[3, :, :, curr_layer] = anchor_y  # height of current anchor

            # Apply regression to x, y, w and h if there is rpn regression layer
            if use_regr:
                A[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)

            # Avoid width and height exceeding 1
            A[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])
            A[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])

            # Convert (x, y , w, h) to (x1, y1, x2, y2)
            # x1, y1 is top left coordinate
            # x2, y2 is bottom right coordinate
            A[2, :, :, curr_layer] += A[0, :, :, curr_layer]
            A[3, :, :, curr_layer] += A[1, :, :, curr_layer]

            # Avoid bboxes drawn outside the feature map
            A[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])
            A[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])
            A[2, :, :, curr_layer] = np.minimum(cols - 1, A[2, :, :, curr_layer])
            A[3, :, :, curr_layer] = np.minimum(rows - 1, A[3, :, :, curr_layer])

            curr_layer += 1

    all_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)
    all_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))  # shape=(4050,)

    x1 = all_boxes[:, 0]
    y1 = all_boxes[:, 1]
    x2 = all_boxes[:, 2]
    y2 = all_boxes[:, 3]

    # Find out the bboxes which is illegal and delete them from bboxes list
    idxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))

    all_boxes = np.delete(all_boxes, idxs, 0)
    all_probs = np.delete(all_probs, idxs, 0)

    # Apply non_max_suppression
    # Only extract the bboxes. Don't need rpn probs in the later process
    result = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]

    return result


def format_img_size(img, C):
    """ formats the image size based on config """
    img_min_side = float(C.im_size)
    (height, width, _) = img.shape

    if width <= height:
        ratio = img_min_side / width
        new_height = int(ratio * height)
        new_width = int(img_min_side)
    else:
        ratio = img_min_side / height
        new_width = int(ratio * width)
        new_height = int(img_min_side)
    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
    return img, ratio


def format_img_channels(img, C):
    """ formats the image channels based on config """
    img = img[:, :, (2, 1, 0)]
    img = img.astype(np.float32)
    img[:, :, 0] -= C.img_channel_mean[0]
    img[:, :, 1] -= C.img_channel_mean[1]
    img[:, :, 2] -= C.img_channel_mean[2]
    img /= C.img_scaling_factor
    img = np.transpose(img, (2, 0, 1))
    img = np.expand_dims(img, axis=0)
    return img


def format_img(img, C):
    """ formats an image for model prediction based on config """
    img, ratio = format_img_size(img, C)
    img = format_img_channels(img, C)
    return img, ratio


# Method to transform the coordinates of the bounding box to its original size
def get_real_coordinates(ratio, x1, y1, x2, y2):
    real_x1 = int(round(x1 // ratio))
    real_y1 = int(round(y1 // ratio))
    real_x2 = int(round(x2 // ratio))
    real_y2 = int(round(y2 // ratio))

    return (real_x1, real_y1, real_x2, real_y2)


# def mainf(image_save_path,filename):
#
#     C.use_horizontal_flips = False
#     C.use_vertical_flips = False
#     C.rot_90 = False

def mainf(image_save_path, filename, C, filename_zonder_extention):
    base_path = 'model_2'

    # test_base_path = '../../data/augmentatie/deel_2_minder_augment'  # Directory to save the test images
    test_base_path = image_save_path
    # config_output_filename = os.path.join(base_path, 'model_resnet_config.pickle')
    #
    # with open(config_output_filename, 'rb') as f_in:
    #     C = pickle.load(f_in)
    print('TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT')
    # turn off any data augmentation at test time
    C.use_horizontal_flips = False
    C.use_vertical_flips = False
    C.rot_90 = False
    # K.clear_session()

    num_features = 1024

    input_shape_img = (None, None, 3)
    input_shape_features = (None, None, num_features)

    # define the RPN, built on the base layers
    # num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)
    # rpn_layers = rpn_layer(shared_layers, num_anchors)

    model_rpn = tf.keras.models.load_model('model_test/my_model_test_rpn')

    model_classifier = tf.keras.models.load_model('model_test/my_model_test_clas')
    model_classifier_only = tf.keras.models.load_model('model_test/my_model_test_clas_only')

    model_rpn.compile(optimizer='sgd', loss='mse')
    model_classifier.compile(optimizer='sgd', loss='mse')
    model_classifier_only.compile(optimizer='sgd', loss='mse')

    class_mapping = C.class_mapping
    class_mapping = {v: k for k, v in class_mapping.items()}

    test_imgs = os.listdir(test_base_path)  # Dit staat in commentaar

    imgs_path = test_imgs
    # for i in range(10):
    #     idx = np.random.randint(len(test_imgs))
    #     # print(idx)
    #     imgs_path.append(test_imgs[idx])

    # print('imgs_path', imgs_path)

    all_imgs = []

    classes = {}

    # If the box classification value is less than this, we ignore this box
    bbox_threshold = 0.965
    print_rpn_boxes = False
    print_boxes = True

    alle_info = []
    for idx, img_name in enumerate(imgs_path):
        if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):
            continue
        print(img_name)
        print('idx', idx)
        tijdelijke_info = {}
        st = time.time()
        filepath = os.path.join(test_base_path, img_name)
        # print(filepath)
        img = cv2.imread(filepath)

        opslag_plaats = 'static/ingepakt/' + filename_zonder_extention + '/' + img_name
        tijdelijke_info['name'] = img_name
        tijdelijke_info['filepath'] = opslag_plaats
        tijdelijke_info['width'] = img.shape[1]
        tijdelijke_info['hight'] = img.shape[0]

        X, ratio = format_img(img, C)
        # print(X)
        # print('ratio', ratio)
        ratio = ratio

        X = np.transpose(X, (0, 2, 3, 1))

        # get output layer Y1, Y2 from the RPN and the feature maps F
        # Y1: y_rpn_cls
        # Y2: y_rpn_regr
        [Y1, Y2, F] = model_rpn.predict(X)

        # Get bboxes by applying NMS
        # R.shape = (300, 4)
        R = rpn_to_roi(Y1, Y2, C, 'tf', overlap_thresh=0.7)
        # print('R.shape', R.shape)
        grootste_x = 0
        grootste_y = 0
        tel = 0

        if (print_rpn_boxes == True):
            aantal_boxen = 0
            for co in R:
                if aantal_boxen < 20:
                    (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio * 0.0625, co[0], co[1], co[2],
                                                                                co[3])
                    # print('real_x1',real_x1)
                    # print('real_x2',real_x2)
                    # print('co[2]',co[2])
                    #         if grootste_x < co[2]:
                    #             grootste_x=co[2]
                    #         if grootste_y < co[3]:
                    #             grootste_y=co[3]
                    tel += 1
                    # print('co',co)
                    # print('class_to_color',class_to_color)
                    # print('int(class_to_color[huidmondje][0])',class_to_color['huidmond'][0])

                    kleur_3 = 60
                    kleur_1 = 254 - tel
                    kleur_2 = tel
                    if kleur_1 < 0:
                        kleur_1 = 0
                        kleur_2 = 254
                        kleur_3 = tel - 200

                    cv2.rectangle(img, (real_x1, real_y1), (real_x2, real_y2), (kleur_3, kleur_1, kleur_2), 6)
                aantal_boxen += 1
        # convert from (x1,y1,x2,y2) to (x,y,w,h)
        R[:, 2] -= R[:, 0]
        R[:, 3] -= R[:, 1]

        tel_2 = 0
        # apply the spatial pyramid pooling to the proposed regions
        bboxes = {}
        probs = {}

        # print('R.shape:', R.shape[0])
        # print('range(R.shape):', range(R.shape[0]))
        # print('C.num_rios:', C.num_rois)
        # print('range(alles):', range(R.shape[0] // C.num_rois + 1))

        # R.shape[0] = (0,300)
        # C.num_rios = 4
        # 300/4 = 75 -> 75+1=76
        # Dus hier zal een for lus 76 keer ittereren
        # num_rios = 4 dit betekent dat we 4 rios gaan bekijken tegelijk

        for jk in range(R.shape[0] // C.num_rois + 1):
            tel_2 += 1
            ROIs = np.expand_dims(R[C.num_rois * jk:C.num_rois * (jk + 1), :], axis=0)
            # ROIs.shape[1] is normaal gelijk aan 4
            if ROIs.shape[1] == 0:
                break

            # print('jk',jk)
            # print('R.shape[0]//C.num_rois: ',R.shape[0]//C.num_rois)

            # hierin komen we nooit DIT doet ook niets
            # Hier zal jk optellen van 0 tot 74
            #  R.shape[0]//C.num_rois = 75    dit is constant het geval
            if jk == R.shape[0] // C.num_rois:
                # pad R
                curr_shape = ROIs.shape
                target_shape = (curr_shape[0], C.num_rois, curr_shape[2])
                ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)
                ROIs_padded[:, :curr_shape[1], :] = ROIs
                ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]
                ROIs = ROIs_padded

            # print('F.shape',F.shape)
            # print('RIO.shape',ROIs.shape)
            [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])

            # print('P_cls: ',P_cls)
            # print('P_regr: ',P_regr)


            # Calculate bboxes coordinates on resized image
            for ii in range(P_cls.shape[1]):
                # Ignore 'bg' class
                # print('P_cls[0, ii, :]: ',P_cls[0, ii, :])

                # P_cls[0, ii, :]:  [0.00348739 0.9965126 ]
                # P_cls[0, ii, :]:  [0.03592544 0.9640745 ]
                #  [% kan dat dit een huidmondje is, % kans dat het background (bg) is ]
                #  [X,Y] hier is X+Y=100%
                # P_cls[0, ii, :]:   geeft dus voorspellingen
                # FOR IF CONTINUE: Wanneer het if statement true is
                # zal er overgegaan worden naar de volgende itteratie van de for lus
                # DUS om door te gaan moet het if statement False zijn
                # print('tel_2: ',tel_2)

                # Stop als het voorspeelde % te laag is
                if np.max(P_cls[0, ii, :]) < bbox_threshold:
                    continue

                # We zullen hier stoppen als de kans op huidsel<50%
                #             if P_cls[0, ii, :][0] < 0.5:
                #                 continue

                # np.argmax(P_cls[0, ii, :])=1
                # argmax zal de index teruggeven van het hoogste getal
                # Dus als dit 1 is dan betekent dat in [x,y] nu y het grootste getal zal zijn

                # P_cls.shape[2] - 1 = 1
                # Deze zijn vaak hetzelfde. Dan wordt er dus niet doorgegaan

                if np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):
                    continue
                # print('tel_2: ', tel_2)
                # print('P_cls[0, ii, :]: ', P_cls[0, ii, :])
                # # print(P_cls[0, ii, :])
                # print('np.arg', np.argmax(P_cls[0, ii, :]))
                # print('C.class_mapping', C.class_mapping)
                cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]
                #             print('KKKK')
                # print('cls_name: ', cls_name)

                if cls_name not in bboxes:
                    bboxes[cls_name] = []
                    probs[cls_name] = []

                (x, y, w, h) = ROIs[0, ii, :]

                cls_num = np.argmax(P_cls[0, ii, :])
                try:
                    (tx, ty, tw, th) = P_regr[0, ii, 4 * cls_num:4 * (cls_num + 1)]
                    tx /= C.classifier_regr_std[0]
                    ty /= C.classifier_regr_std[1]
                    tw /= C.classifier_regr_std[2]
                    th /= C.classifier_regr_std[3]
                    x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)
                except:
                    pass
                bboxes[cls_name].append(
                    [C.rpn_stride * x, C.rpn_stride * y, C.rpn_stride * (x + w), C.rpn_stride * (y + h)])
                probs[cls_name].append(np.max(P_cls[0, ii, :]))
        # print('tel_2', tel_2)
        all_dets = []
        tijdelijke_info['bboxen'] = []

        for key in bboxes:
            bbox = np.array(bboxes[key])
            # print('t')
            # print(bbox)
            new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.2)
            #         print('new_boxes',new_boxes)

            for jk in range(new_boxes.shape[0]):
                (x1, y1, x2, y2) = new_boxes[jk, :]

                # Calculate real coordinates on original image
                (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)
                #             print(real_x1, real_y1, real_x2, real_y2)
                cv2.rectangle(img, (real_x1, real_y1), (real_x2, real_y2), (250, 0, 0),
                              4)  # (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),4)

                textLabel = '{}'.format(int(100 * new_probs[jk]))  # '{}: {}'.format(key,int(100*new_probs[jk]))
                all_dets.append((key, 100 * new_probs[jk]))
                # print('test')

                (retval, baseLine) = cv2.getTextSize(textLabel, cv2.FONT_HERSHEY_COMPLEX, 1, 1)
                textOrg = (real_x1, real_y1 - 0)

                cv2.rectangle(img, (textOrg[0] - 5, textOrg[1] + baseLine - 5),
                              (textOrg[0] + retval[0] + 5, textOrg[1] - retval[1] - 5), (0, 0, 0), 1)
                cv2.rectangle(img, (textOrg[0] - 5, textOrg[1] + baseLine - 5),
                              (textOrg[0] + retval[0] + 5, textOrg[1] - retval[1] - 5), (255, 255, 255), -1)
                cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)
                tijdelijke_info['bboxen'].append(
                    {'class': key, 'x1': real_x1, 'x2': real_x2, 'y1': real_y1, 'y2': real_y2,
                     'prob': 100 * new_probs[jk]})

        print('Elapsed time = {}'.format(time.time() - st))
        print(all_dets)
        alle_info.append(tijdelijke_info)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        # plt.figure(figsize=(8, 8))
        # plt.grid()
        # plt.imshow(img)
        add_afbeelding_to_map(img, filename_zonder_extention, img_name)
        # plt.show()
        # plt.imsave('/home/nitin/Desktop/flask-api-upload-image/uploads/test.png', img)
    K.clear_session()
    return alle_info


def add_afbeelding_to_map(image, filename, image_name):
    opslag_path = 'static/ingepakt/' + filename
    image_opslag_path = opslag_path + '/' + image_name
    print('opslag path', image_opslag_path)
    cv2.imwrite(image_opslag_path, image)
    # image.safe(image_opslag_path)


def zip_de_afbeeldingen(filename):
    print('filename zip afbeelding', filename)
    opslag_path = 'static/aan_elkaar/' + filename + '_gezipt'
    output_filename = 'static/aan_elkaar/' + filename
    shutil.make_archive(opslag_path, 'zip', output_filename)
    # opslag_path.seek(0)


def return_zip_afbeeldingen(filename):
    opslag_path = 'static/aan_elkaar/' + filename + '_gezipt.zip'

    print('return zip opslag path', opslag_path)
    output_filename = filename + '_gezipt.zip'
    # output_filename.seek(0)
    print('image_name')
    # as_attachment zorgt ervoor dat je het tekst document download
    # return send_file('static/text_files/tekst.txt', attachment_filename='tekst.txt', as_attachment=True)
    result = send_file(opslag_path, attachment_filename=output_filename, as_attachment=True)
    # os.remove(opslag_path)
    return result


def write_to_file(text_path, all_data, filename_zonder_extention):
    text_path = 'static/aan_elkaar/' + filename_zonder_extention + '/' + text_path + '.txt'
    text_w = open(text_path, "w+")
    text_w.write('naam_plantsoort,x1_coordinaten,x2_coordinaten,y1_coordinaten,y2_coordinaten,probabiliteit_stomata' + '\n')
    for namen in all_data:
        for bboxen in namen['bboxen']:
            text_w.write(namen['name'][4:] + ',' + bboxen['class'] + ',' + str(int(bboxen['x1'])) + ',' + str(
                int(bboxen['x2'])) + ',' + str(int(bboxen['y1'])) + ',' + str(int(bboxen['y2'])) + ',' + str(
                bboxen['prob']) + '\n')
            # [{'class': 'huidmond', 'x1': 256, 'x2': 416, 'y1': 368, 'y2': 512, 'prob': 99.06604886054993}]}]
    # text_w.write('hallo het is gelukt om te schrijven')
    #
    text_w.close()


def extract_zip(file_path):
    # Create a ZipFile Object and load sample.zip in it
    print('begin exract_zip')
    print('file_path', file_path)
    with ZipFile(file_path, 'r') as zipObj:
        # Extract all the contents of zip file in current directory
        zipObj.extractall('static/uitgepakt/')
    os.remove(file_path)
    print('EINDE')


def add_uitgesneden_afbeelding_to_map(image, filenaam, image_name):
    # filename = in_stukken
    opslag_path = 'static/in_stukken/' + filenaam
    # print('opslag_path',opslag_path)
    image_opslag_path = opslag_path + '/' + image_name
    # print('opslag path_foto',image_opslag_path)

    cv2.imwrite(image_opslag_path, image)
    # image.safe(image_opslag_path)


def find_gamma(image):
    """
        Geeft de correcte gamma terug zodat de intensiteit van de afbeelding naar het midden wordt verschoven
        125: Het midden van de intensiteit
        Arguments:
            image - een foto

        Returns:
            gamma {int} -- De gamma
    """
    avg_intensity = np.mean(image)
    tus_1 = np.log(125 / 255)
    tus_2 = np.log(avg_intensity / 255)
    inVgamma = tus_1 / tus_2
    gamma = 1 / inVgamma
    return gamma


def adjust_gamma(image, gamma=1.0):
    """
        Build a lookup table mapping the pixel values [0, 255] to their adjusted gamma values
        Arguments:
            image - een foto
            gamma {float} -- De gamma voor de augmentatie

        Returns:
            De lookup table
    """
    invGamma = 1.0 / gamma
    table = np.array([((i / 255.0) ** invGamma) * 255
                      for i in np.arange(0, 256)]).astype("uint8")

    return cv2.LUT(image, table)

def normalize(image):
    # All deze booleans worden niet gebruikt. Als je handmatig naar de histograms wilt kijken kunnen ze handig zijn

    clip_limit = False
    histogram_equal = False

    # histogram_equalization(image)

    clahe2 = cv2.createCLAHE(clipLimit=1.4, tileGridSize=(32, 32))
    cl2 = clahe2.apply(image)
    if clip_limit == True:
        # Dit is alleen gebruikt voor in het verslag. Hier gaan we addaptive histogram equalisation vergelijken met het gewone histogram equalization
        clahe2 = cv2.createCLAHE(clipLimit=40, tileGridSize=(32, 32))
        adjusted2 = clahe2.apply(image)
    elif histogram_equal == True:
        adjusted2 = cv2.equalizeHist(image)
    else:
        gamma = find_gamma(cl2)
        adjusted2 = adjust_gamma(cl2, gamma=gamma)
    # show_img_hist_cum_titels_1(adjusted2)
    # show_img_hist_cum_titels_3(image,cl2,adjusted2)
    # show_img_hist_cum_titels(image,cl2,adjusted2)

    # show_img_hist_cum_x([image,cl2,adjusted2])

    return adjusted2

def opdelen_stukken(foto_info, image, filenaam):
    # foto_info['info_stukken'] = []
    tijdelijke_info = []
    y, x = image.shape
    # image = image[0:y,0:x,1]
    x_richting = 0
    while x_richting < foto_info['aantal_x_stukken']:
        y_richting = 0
        while y_richting < foto_info['aantal_y_stukken']:
            # print('y_richting',y_richting)
            if y_richting == 0:
                y1 = 0
                y2 = 600
            else:
                y1 = y - 600
                y2 = y
            if x_richting == 0:
                x1 = 0
                x2 = 800
            else:
                x1 = x - 800
                x2 = x

            nieuwe_imgage = image[y1:y2, x1:x2]

            nieuwe_imgage = normalize(nieuwe_imgage)


            # plt.imshow(nieuwe_imgage)
            # plt.show()
            nieuwe_naam = str(x_richting) + '_' + str(y_richting) + '_' + foto_info['originele_naam']
            tijdelijke_info.append(
                {'nieuwe_naam': nieuwe_naam, 'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'x_richting': x_richting,
                 'y_richting': y_richting})
            # print('HIER INFO STUKKEn',foto_info['info_stukken'])
            # print('HIER INFO',foto_info)
            add_uitgesneden_afbeelding_to_map(nieuwe_imgage, filenaam, nieuwe_naam)
            # Nu deze nieuwe image opslaan
            y_richting = y_richting + 1
            # En ook de naam ervan opslaan
        x_richting += 1
    # print('FOTO INFO STUKKEN',foto_info)
    return tijdelijke_info


def open_foto(all_data, data_locatie_foto, filenaam):
    locatie_foto = data_locatie_foto + '' + filenaam
    alle_fotos_info = []
    for naam_foto in all_data:
        foto_info = {}
        print(naam_foto)
        data_path_foto_test = locatie_foto + naam_foto

        foto_info['originele_naam'] = naam_foto
        foto_info['filepath'] = data_path_foto_test

        image = cv2.imread(data_path_foto_test)
        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        y, x = image.shape

        if x < y:
            image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)
            x_tussen = x
            x = y
            y = x_tussen
        # print('y',y)
        # print('x',x)
        # y=1250
        y_delen = y / 600
        y_delen = int(np.ceil(y_delen))
        # print('y_delen',y_delen)

        x_delen = x / 800
        x_delen = int(np.ceil(x_delen))
        # print('x_delen',x_delen)
        foto_info['aantal_x_stukken'] = x_delen
        foto_info['aantal_y_stukken'] = y_delen
        foto_info['originele_x_grootte'] = x
        foto_info['originele_y_grootte'] = y
        uiteindelijke_info = opdelen_stukken(foto_info, image, filenaam)

        foto_info['info_stukken'] = uiteindelijke_info
        # print('info_stukken',uiteindelijke_info)
        alle_fotos_info.append(foto_info)
        # print('alle_fotos_info',alle_fotos_info)
        # print('foto_info',foto_info)

        # plt.imshow(image)
        # plt.show()
    return alle_fotos_info


def overlopen_fotos(data_locatie, filenaam):
    locatie = data_locatie + '' + filenaam
    print('locatie', locatie)
    test_imgs = os.listdir(locatie)
    # print('test_imgs',test_imgs)
    alle_fotos_info = open_foto(test_imgs, data_locatie, filenaam)
    return alle_fotos_info


def zoek_index_lijst(naam_deel_afbeelding, super_test):
    for naam in super_test:
        if naam_deel_afbeelding == naam['name']:
            return naam
    return False


def zet_een_foto_aan_elkaar(een_afbeelding, super_test):
    blank_image = np.zeros(shape=[een_afbeelding['originele_y_grootte'], een_afbeelding['originele_x_grootte'], 3])
    # plt.imshow(blank_image)
    # plt.show()
    # print('een_afbeelding',een_afbeelding)
    # print('een_afbeelding[originele_y_grootte]',een_afbeelding['originele_y_grootte'])
    # print('een_afbeelding[info_stukken]',een_afbeelding['info_stukken'])
    for deel_afbeelding in een_afbeelding['info_stukken']:
        # print('deel_afbeelding:',deel_afbeelding)
        stuk_super_test = zoek_index_lijst(deel_afbeelding['nieuwe_naam'], super_test)
        print('stuk_super_test', stuk_super_test)
        print('deel_afbeelding', deel_afbeelding)
        print('stuk_super_test[filepath]', stuk_super_test['filepath'])
        img_deel = cv2.imread(stuk_super_test['filepath'])
        # plt.imshow(img_deel)
        # plt.show()
        # img_deel = cv2.cvtColor(img_deel, cv2.COLOR_BGR2GRAY)
        print('shape', img_deel.shape)
        # plt.imshow(img_deel,cmap='gray', vmin=0, vmax=255)
        # plt.show()
        y1 = deel_afbeelding['y1']
        y2 = deel_afbeelding['y2']
        x1 = deel_afbeelding['x1']
        x2 = deel_afbeelding['x2']

        blank_image[y1:y2, x1:x2] = img_deel

    # plt.imshow(blank_image,cmap='gray', vmin=0, vmax=255)
    # plt.show()

    print('t')
    return blank_image


def sla_aan_elkaar_op(image, filenaam, image_name):
    # filename = in_stukken
    opslag_path = 'static/aan_elkaar/' + filenaam
    # print('opslag_path',opslag_path)
    image_opslag_path = opslag_path + '/' + image_name
    cv2.imwrite(image_opslag_path, image)


def zet_fotos_aan_elkaar(alle_fotos_info, super_test, filenaam):
    print('c')
    for een_afbeelding in alle_fotos_info:
        blank_image = zet_een_foto_aan_elkaar(een_afbeelding, super_test)
        # plt.imshow(blank_image,cmap='gray', vmin=0, vmax=255)
        # plt.show()
        sla_aan_elkaar_op(blank_image, filenaam, een_afbeelding['originele_naam'])


C = Config()


def maint(file_path, filename, C):
    # Config = C
    array_filename_zonder_extention = filename.split('.')
    filename_zonder_extention = array_filename_zonder_extention[0]
    print('filename_zonder_extention', filename_zonder_extention)
    uitpakplaats = 'static/uitgepakt/' + filename_zonder_extention

    print('uitpakplaats', uitpakplaats)
    print('fale_path', file_path)
    extract_zip(file_path)
    # zip_file = 'static/upload/' + filename_zonder_extention + '.zip'
    # zip_file.close()
    # os.remove('static/upload/' + filename_zonder_extention + '.zip')

    # Heel dit deel is om de images op te slaan op plaats static/ingepakt/zip_name
    os.mkdir('static/ingepakt/' + filename_zonder_extention)
    os.mkdir('static/in_stukken/' + filename_zonder_extention)
    os.mkdir('static/aan_elkaar/' + filename_zonder_extention)
    # time.sleep(5)
    data_locatie = 'static/uitgepakt/'
    print('filename eerste', filename)
    filename = filename_zonder_extention + '/'

    print('filename tweede', filename)
    alle_fotos_info = overlopen_fotos(data_locatie, filename)

    test_imgs = os.listdir(uitpakplaats)
    in_stukken_uitgepatk = 'static/in_stukken/' + filename_zonder_extention
    super_test = mainf(in_stukken_uitgepatk, filename, C, filename_zonder_extention)

    print('super_test', super_test)
    print('alle_fotos_info', alle_fotos_info)

    zet_fotos_aan_elkaar(alle_fotos_info, super_test, filename)

    write_to_file(filename_zonder_extention, super_test, filename_zonder_extention)

    zip_de_afbeeldingen(filename_zonder_extention)

    zip_file = return_zip_afbeeldingen(filename_zonder_extention)
    print('GEDAAAAAAAN')

    shutil.rmtree('static/ingepakt/' + filename_zonder_extention)
    shutil.rmtree('static/in_stukken/' + filename_zonder_extention)
    shutil.rmtree('static/aan_elkaar/' + filename_zonder_extention)
    shutil.rmtree('static/uitgepakt/' + filename_zonder_extention)
    # shutil.rmtree('static/upload/' + filename_zonder_extention + '.zip')
    # os.remove('static/aan_elkaar/' + filename_zonder_extention + '.zip')
    print('test')
    return zip_file


''' Hier opgeslagen van alle dingen die werken zoals foto's opslaan enzo. Mss gebruiken om te tonen aan Francis'''
# def maint(file_path, filename):
#     array_filename_zonder_extention = filename.split('.')
#     filename_zonder_extention = array_filename_zonder_extention[0]
#     print('filename_zonder_extention', filename_zonder_extention)
#     uitpakplaats = 'static/uitgepakt/' + filename_zonder_extention
#
#     print('uitpakplaats', uitpakplaats)
#     print('fale_path',file_path)
#     extract_zip(file_path)
#     test_imgs = os.listdir(uitpakplaats)
#
#     # Heel dit deel is om de images op te slaan op plaats static/ingepakt/zip_name
#
#     os.mkdir('static/ingepakt/' + filename_zonder_extention)
#     for idx, img_name in enumerate(test_imgs):
#         if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):
#             continue
#         print('img_name', img_name)
#         print('idx', idx)
#         img_path = os.path.join(uitpakplaats, img_name)
#         img = cv2.imread(img_path)
#         # plt.imshow(img)
#         # plt.show()
#         add_afbeelding_to_map(img, filename_zonder_extention, img_name)
#
#     # Nu gaan we de opgeslagen images inpakken met een zip
#     zip_de_afbeeldingen(filename_zonder_extention)
#
#     zip_file = return_zip_afbeeldingen(filename_zonder_extention)
#     print('GEDAAAAAAAN')
#
#     # mainf(uitpakplaats,filename)
#     # all_data=[{'name': 'LB_H_1_Cyrtorchis chailluara Pauwels 4176_blad3-vlak3.jpg', 'filepath': '../../data/augmentatie/deel_2_minder_augment/LB_H_1_Cyrtorchis chailluara Pauwels 4176_blad3-vlak3.jpg', 'width': 600, 'hight': 800, 'bboxen': [{'class': 'huidmond', 'x1': 256, 'x2': 416, 'y1': 368, 'y2': 512, 'prob': 99.06604886054993}]}]
#     all_data = [{'name': 'RB_H_V_Ipomaea eriocarpa Faulkner 3424_blad1-vlak1.jpg',
#                  'filepath': '../../data/augmentatie/deel_2_minder_augment/RB_H_V_Ipomaea eriocarpa Faulkner 3424_blad1-vlak1.jpg',
#                  'width': 800, 'hight': 600,
#                  'bboxen': [{'class': 'huidmond', 'x1': 528, 'x2': 672, 'y1': 0, 'y2': 128, 'prob': 99.8534083366394},
#                             {'class': 'huidmond', 'x1': 0, 'x2': 128, 'y1': 432, 'y2': 576, 'prob': 99.80648159980774},
#                             {'class': 'huidmond', 'x1': 544, 'x2': 704, 'y1': 336, 'y2': 480,
#                              'prob': 99.52600598335266},
#                             {'class': 'huidmond', 'x1': 160, 'x2': 352, 'y1': 208, 'y2': 320,
#                              'prob': 98.63857626914978}]}]
#     write_to_file('test', all_data)
#     print('test')
#     return zip_file
